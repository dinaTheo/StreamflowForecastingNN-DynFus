{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 12:01:58.871329: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from hydroeval import evaluator, nse, kge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = np.load(\"Results/Saved-from-run-models/durations-encdec-dyn-fusion.npy\")\n",
    "durations_for_preds = np.load(\"Results/Saved-from-run-models/durations-for-predictions-encdec-dyn-fusion.npy\")\n",
    "predictions_per_run = np.load(\"Results/Saved-from-run-models/predictions-per-run-encdec-dyn-fusion.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stations=6\n",
    "n_runs=10\n",
    "def ensemble_predict(predictions): \n",
    "    predictions = np.array(predictions)\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    return avg_predictions\n",
    "final_ensemble_predictions = []\n",
    "for station_idx in range(n_stations):\n",
    "    station_predictions_across_runs = [predictions_per_run[run][station_idx] for run in range(n_runs)]\n",
    "    station_ensemble = ensemble_predict(station_predictions_across_runs)\n",
    "    final_ensemble_predictions.append(station_ensemble)\n",
    "np.save(\"Results/Saved-from-run-models/e-predictions-per-run-base-dyn1-fusion.npy\", final_ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/th/mt68jv4d1yg0fk5k079pdmdr0000gn/T/ipykernel_2319/167017545.py:12: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  climate_variables = climate_variables.interpolate(method='linear', limit_direction='both', axis=0)\n"
     ]
    }
   ],
   "source": [
    "folder_with_data = 'Data/Final-data/'\n",
    "\n",
    "river_flow = pd.read_csv(folder_with_data + '/river-flow.csv')\n",
    "river_flow['dateTime'] = pd.to_datetime(river_flow['dateTime'], errors='raise')\n",
    "river_flow['dateTime'] = river_flow['dateTime'].dt.date\n",
    "river_flow = river_flow.groupby('dateTime').mean().reset_index()\n",
    "\n",
    "climate_variables = pd.read_csv(folder_with_data + '/atmospheric-variables.csv')\n",
    "climate_variables['dateTime'] = pd.to_datetime(climate_variables['dateTime'], errors='raise')\n",
    "climate_variables['dateTime'] = climate_variables['dateTime'].dt.date\n",
    "climate_variables = climate_variables.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "\n",
    "river_flow = river_flow.drop(columns={'Unnamed: 0'})\n",
    "weather_data = pd.merge(climate_variables, river_flow, on=['dateTime'], how='inner')\n",
    "\n",
    "weather_data = weather_data.iloc[:,[0,3,1,4,5,6,7,8,9,10,11,12,13]]\n",
    "time_steps = 30 \n",
    "horizon = 7 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 30 \n",
    "horizon = 7 \n",
    "\n",
    "numerical_columns = weather_data.select_dtypes(include=['float64']).columns\n",
    "datetime_columns = weather_data.select_dtypes(include=['datetime64']).columns\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "weather_data[numerical_columns]= pd.DataFrame(scaler.fit_transform(weather_data[numerical_columns]), columns = numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y =  weather_data.iloc[:, 8:] \n",
    "Y1 =  weather_data.iloc[:,8] \n",
    "Y2 =  weather_data.iloc[:,9] \n",
    "Y3 =  weather_data.iloc[:,10] \n",
    "Y4 =  weather_data.iloc[:,11] \n",
    "Y5 =  weather_data.iloc[:,12] \n",
    "Y6 =  weather_data.iloc[:,13] \n",
    "X1 = weather_data.iloc[:, [1]]  \n",
    "X2 = weather_data.iloc[:, [2]] \n",
    "X3 = weather_data.iloc[:, [3]] \n",
    "X4 = weather_data.iloc[:, [4]] \n",
    "X5 = weather_data.iloc[:, [5]] \n",
    "X6 = weather_data.iloc[:, [6]] \n",
    "X7 = weather_data.iloc[:, [7]] \n",
    "time = weather_data.iloc[:, [0]] \n",
    "\n",
    "def create_sequences(X1, X2, X3, X4, X5, X6, \n",
    "                     X7,\n",
    "                     Y, time, time_steps, horizon):\n",
    "    \n",
    "    X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, X7_seq = [], [], [], [], [], [], []\n",
    "    \n",
    "    Y_past_seq = []\n",
    "    \n",
    "    Y_seq = []\n",
    "    \n",
    "    time_seq = []\n",
    "\n",
    "    for i in range(len(Y) - time_steps - horizon + 1):\n",
    "        X1_seq.append(X1[i:i+time_steps])\n",
    "        X2_seq.append(X2[i:i+time_steps])\n",
    "        X3_seq.append(X3[i:i+time_steps])\n",
    "        X4_seq.append(X4[i:i+time_steps])\n",
    "        X5_seq.append(X5[i:i+time_steps])\n",
    "        X6_seq.append(X6[i:i+time_steps])\n",
    "        X7_seq.append(X7[i:i+time_steps])\n",
    "\n",
    "        Y_past_seq.append(Y.iloc[i:i+time_steps]) \n",
    "\n",
    "        Y_seq.append(Y.iloc[i+time_steps:i+time_steps+horizon])  \n",
    "\n",
    "        time_seq.append(time[i:i+time_steps])\n",
    "\n",
    "\n",
    "    X1_seq = np.array(X1_seq)\n",
    "    X2_seq = np.array(X2_seq)\n",
    "    X3_seq = np.array(X3_seq)\n",
    "    X4_seq = np.array(X4_seq)\n",
    "    X5_seq = np.array(X5_seq)\n",
    "    X6_seq = np.array(X6_seq)\n",
    "    X7_seq = np.array(X7_seq)\n",
    "    \n",
    "    Y_past_seq = np.array(Y_past_seq)\n",
    "    \n",
    "    Y_seq = np.array(Y_seq)\n",
    "    \n",
    "    time_seq = np.array(time_seq)\n",
    "\n",
    "    return (X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq,\n",
    "            X7_seq,\n",
    "            Y_past_seq,\n",
    "            Y_seq, time_seq)\n",
    "\n",
    "# Assuming time_steps and horizon are defined\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, X7_seq, Y1_past_seq, Y1_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, X7, Y1, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, X7_seq, Y2_past_seq, Y2_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, X7, Y2, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, X7_seq, Y3_past_seq, Y3_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, X7, Y3, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, X7_seq, Y4_past_seq, Y4_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, X7, Y4, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq,  X7_seq, Y5_past_seq, Y5_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, X7, Y5, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq,  X7_seq, Y6_past_seq, Y6_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, X7, Y6, time, time_steps, horizon\n",
    ")\n",
    "\n",
    "modality_1 = Y1_past_seq\n",
    "modality_2 = Y2_past_seq\n",
    "modality_3 = Y3_past_seq\n",
    "modality_4 = Y4_past_seq\n",
    "modality_5 = Y5_past_seq\n",
    "modality_6 = Y6_past_seq\n",
    "\n",
    "# Split into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, X7_train, X7_test, y1_past_train, y1_past_test,  y1_train, y1_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq,  X7_seq, Y1_past_seq, Y1_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, X7_train, X7_test, y2_past_train, y2_past_test,  y2_train, y2_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq,  X7_seq, Y2_past_seq, Y2_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, X7_train, X7_test, y3_past_train, y3_past_test,  y3_train, y3_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq,  X7_seq, Y3_past_seq, Y3_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, X7_train, X7_test, y4_past_train, y4_past_test,  y4_train, y4_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq,  X7_seq, Y4_past_seq, Y4_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, X7_train, X7_test, y5_past_train, y5_past_test,  y5_train, y5_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq,  X7_seq, Y5_past_seq, Y5_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, X7_train, X7_test, y6_past_train, y6_past_test,  y6_train, y6_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq,  X7_seq, Y6_past_seq, Y6_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_time, test_time = train_test_split(time_seq, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y =  weather_data.iloc[:, 7:]  \n",
    "Y1 =  weather_data.iloc[:,7] \n",
    "Y2 =  weather_data.iloc[:,8] \n",
    "Y3 =  weather_data.iloc[:,9] \n",
    "Y4 =  weather_data.iloc[:,10] \n",
    "Y5 =  weather_data.iloc[:,11] \n",
    "Y6 =  weather_data.iloc[:,12] \n",
    "X1 = weather_data.iloc[:, [1]]  \n",
    "X2 = weather_data.iloc[:, [2]] \n",
    "X3 = weather_data.iloc[:, [3]] \n",
    "X4 = weather_data.iloc[:, [4]] \n",
    "X5 = weather_data.iloc[:, [5]] \n",
    "X6 = weather_data.iloc[:, [6]] \n",
    "time = weather_data.iloc[:, [0]] \n",
    "\n",
    "def create_sequences(X1, X2, X3, X4, X5, X6, \n",
    "                     Y, time, time_steps, horizon):\n",
    "    \n",
    "    X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq = [], [], [], [], [], []\n",
    "    \n",
    "    Y_past_seq = []\n",
    "    \n",
    "    Y_seq = []\n",
    "    \n",
    "    time_seq = []\n",
    "\n",
    "    for i in range(len(Y) - time_steps - horizon + 1):\n",
    "        X1_seq.append(X1[i:i+time_steps])\n",
    "        X2_seq.append(X2[i:i+time_steps])\n",
    "        X3_seq.append(X3[i:i+time_steps])\n",
    "        X4_seq.append(X4[i:i+time_steps])\n",
    "        X5_seq.append(X5[i:i+time_steps])\n",
    "        X6_seq.append(X6[i:i+time_steps])\n",
    "        \n",
    "        Y_past_seq.append(Y.iloc[i:i+time_steps]) \n",
    "\n",
    "        Y_seq.append(Y.iloc[i+time_steps:i+time_steps+horizon])  \n",
    "\n",
    "        time_seq.append(time[i:i+time_steps])\n",
    "\n",
    "    X1_seq = np.array(X1_seq)\n",
    "    X2_seq = np.array(X2_seq)\n",
    "    X3_seq = np.array(X3_seq)\n",
    "    X4_seq = np.array(X4_seq)\n",
    "    X5_seq = np.array(X5_seq)\n",
    "    X6_seq = np.array(X6_seq)\n",
    "    \n",
    "    Y_past_seq = np.array(Y_past_seq)\n",
    "    \n",
    "    Y_seq = np.array(Y_seq)\n",
    "    \n",
    "    time_seq = np.array(time_seq)\n",
    "\n",
    "    return (X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, \n",
    "            Y_past_seq,\n",
    "            Y_seq, time_seq)\n",
    "\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y1_past_seq, Y1_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, Y1, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y2_past_seq, Y2_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, Y2, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y3_past_seq, Y3_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, Y3, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y4_past_seq, Y4_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, Y4, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y5_past_seq, Y5_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, Y5, time, time_steps, horizon\n",
    ")\n",
    "X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y6_past_seq, Y6_seq, time_seq = create_sequences(\n",
    "    X1, X2, X3, X4, X5, X6, Y6, time, time_steps, horizon\n",
    ")\n",
    "\n",
    "modality_1 = Y1_past_seq\n",
    "modality_2 = Y2_past_seq\n",
    "modality_3 = Y3_past_seq\n",
    "modality_4 = Y4_past_seq\n",
    "modality_5 = Y5_past_seq\n",
    "modality_6 = Y6_past_seq\n",
    "\n",
    "# Split into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, y1_past_train, y1_past_test,  y1_train, y1_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y1_past_seq, Y1_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, y2_past_train, y2_past_test,  y2_train, y2_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y2_past_seq, Y2_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, y3_past_train, y3_past_test,  y3_train, y3_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y3_past_seq, Y3_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, y4_past_train, y4_past_test,  y4_train, y4_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y4_past_seq, Y4_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, y5_past_train, y5_past_test,  y5_train, y5_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y5_past_seq, Y5_seq, test_size=0.2, shuffle=False)\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, y6_past_train, y6_past_test,  y6_train, y6_test  = train_test_split(X1_seq, X2_seq, X3_seq, X4_seq, X5_seq, X6_seq, Y6_past_seq, Y6_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_time, test_time = train_test_split(time_seq, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test = []\n",
    "scaler_modified = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_modified.feature_range = scaler.feature_range\n",
    "for j, y_test in enumerate([y1_test, y2_test, y3_test, y4_test, y5_test, y6_test]): \n",
    "    scaler_modified.min_ =  scaler.min_[6+j]\n",
    "    scaler_modified.scale_ =  scaler.scale_[6+j]\n",
    "    scaler_modified.data_min_ =  scaler.data_min_[6+j]\n",
    "    scaler_modified.data_max_ =  scaler.data_max_[6+j]\n",
    "    scaler_modified.data_range_ =  scaler.data_range_[6+j]\n",
    "\n",
    "    y_test = scaler_modified.inverse_transform(y_test)\n",
    "    ys_test.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_encdec_un = np.load('e-predictions-per-run-encdec-dyn-fusion.npy')\n",
    "predictions_encdec_dyn_un = np.load('e-predictions-per-run-encdec-dyn1-fusion.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_encdec_un = np.load('predictions-per-run-encdec-dyn-fusion.npy')[8]\n",
    "predictions_encdec_dyn_un = np.load('predictions-per-run-encdec-dyn1-fusion.npy')[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_per_model = []\n",
    "scaler_modified = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_modified.feature_range = scaler.feature_range\n",
    "for model in [predictions_encdec_un,predictions_encdec_dyn_un]:\n",
    "    predictions = []\n",
    "    for j, model_per_station in enumerate(model): \n",
    "        scaler_modified.min_ =  scaler.min_[6+j]\n",
    "        scaler_modified.scale_ =  scaler.scale_[6+j]\n",
    "        scaler_modified.data_min_ =  scaler.data_min_[6+j]\n",
    "        scaler_modified.data_max_ =  scaler.data_max_[6+j]\n",
    "        scaler_modified.data_range_ =  scaler.data_range_[6+j]\n",
    "        model_per_station = model_per_station.reshape(-1,1)\n",
    "        prediction = scaler_modified.inverse_transform(model_per_station)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    predictions_per_model.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_combined = np.stack(ys_test, axis=1).transpose(0,2,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_combined = y_test_combined.reshape(-1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = y_test_combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_per_run =[predictions_encdec_un,predictions_encdec_dyn_un]\n",
    "predictions = []\n",
    "for prediction in predictions_per_run:\n",
    "        predictions_per_run2 = []\n",
    "        for j in range(prediction.shape[0]): \n",
    "            data_slice = prediction[j, :, :, 0]\n",
    "            scaler_modified.min_ = scaler.min_[6+j]\n",
    "            scaler_modified.scale_ = scaler.scale_[6+j]\n",
    "            scaler_modified.data_min_ = scaler.data_min_[6+j]\n",
    "            scaler_modified.data_max_ = scaler.data_max_[6+j]\n",
    "            scaler_modified.data_range_ = scaler.data_range_[6+j]\n",
    "\n",
    "            data_inverse_transformed = scaler_modified.inverse_transform(data_slice)\n",
    "        \n",
    "            predictions_per_run2.append(data_inverse_transformed)\n",
    "        predictions.append(predictions_per_run2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mase(actual, predicted, m):\n",
    "    n = len(actual)\n",
    "    naive_error = np.abs(actual[m:] - actual[:-m])  \n",
    "    forecast_error = np.abs(actual[m:] - predicted[m:])\n",
    "    scaled_error = np.mean(forecast_error) / np.mean(naive_error)\n",
    "    return scaled_error\n",
    "\n",
    "def nash_sutcliffe_efficiency(observed, predicted):\n",
    "    observed = np.array(observed)\n",
    "    predicted = np.array(predicted)\n",
    "\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    denominator = np.sum((observed - np.mean(observed)) ** 2)\n",
    "\n",
    "    return 1 - (numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Model    MAE   NSE  MASE  MAE_HIGH  \\\n",
      "0   LSTM-EncDec-Intra-Inter-Attention  27.73  0.75  0.53    109.59   \n",
      "1             LSTM-EncDec-Dynamic-Fus  26.29  0.76  0.50    105.62   \n",
      "2  1DCNN-BiLSTM-Intra-Inter-Attention  28.22  0.75  0.54    111.07   \n",
      "3            1DCNN-BiLSTM-Dynamic-Fus  26.32  0.76  0.50    106.85   \n",
      "\n",
      "   RQE_HIGH_90  RQE_HIGH_95  RQE_HIGH_99  \n",
      "0         0.13         0.15         0.18  \n",
      "1         0.14         0.14         0.17  \n",
      "2         0.15         0.16         0.18  \n",
      "3         0.15         0.14         0.17  \n"
     ]
    }
   ],
   "source": [
    "num_stations = 6\n",
    "error_metrics = []\n",
    "mae_high_per_model = []\n",
    "mae_per_model = []\n",
    "nse_per_model = []\n",
    "mase_per_model = []\n",
    "\n",
    "models = [\"LSTM-EncDec-Att-Fus\",\n",
    "          \"LSTM-EncDec-Op-Lev-Fus\"]\n",
    "\n",
    "for model_name, model in zip(models,predictions_per_model):\n",
    "    model =  np.array(model)\n",
    "    model_predictions = model.transpose(1,2,0).reshape(-1,6)\n",
    "\n",
    "    df = pd.DataFrame({'timestamp': range(0,len(actuals.flatten())),'Flow': actuals.flatten()})\n",
    "\n",
    "    threshold_high = df['Flow'].quantile(0.95)  \n",
    "    threshold_low = df['Flow'].quantile(0.05)   \n",
    "\n",
    "    extreme_highs = df[df['Flow'] >= threshold_high]\n",
    "    extreme_lows = df[df['Flow'] <= threshold_low]\n",
    "\n",
    "    df2 = pd.DataFrame({'timestamp': range(0,len(model_predictions.flatten()+1)),'Flow': model_predictions.flatten() })\n",
    "\n",
    "    threshold_high2 = df['Flow'].quantile(0.95)  \n",
    "    threshold_low2 = df['Flow'].quantile(0.05)   \n",
    "\n",
    "    extreme_highs2 = df2[df['Flow'] >= threshold_high2]\n",
    "    extreme_lows2 = df2[df['Flow'] <= threshold_low2]\n",
    "\n",
    "    mae = round(mean_absolute_error(np.array(df['Flow']), np.array(df2['Flow'])),2)\n",
    "    nse = round(nash_sutcliffe_efficiency(np.array(df['Flow']), np.array(df2['Flow'])),2)\n",
    "    mase = round(calculate_mase(np.array(df['Flow']), np.array(df2['Flow']),20),2)\n",
    "    mae_high = round(mean_absolute_error(extreme_highs['Flow'], extreme_highs2['Flow']),2)\n",
    "    \n",
    "    mae_high_per_model.append(mae_high)\n",
    "    mase_per_model.append(mase)\n",
    "    mae_per_model.append(mae)\n",
    "    nse_per_model.append(nse)\n",
    "\n",
    "    error_metrics_df = pd.DataFrame({\n",
    "        'Model' : [model_name],\n",
    "        'MAE' : [mae],\n",
    "        'NSE' : [nse],\n",
    "        'MASE': [mase],\n",
    "        'MAE_HIGH': [mae_high]\n",
    "    })\n",
    "\n",
    "    error_metrics.append(error_metrics_df)\n",
    "\n",
    "    \n",
    "error_metrics = pd.concat(error_metrics,axis=0).reset_index(drop=True)\n",
    "print(error_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics.to_csv('error_metrics_un.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Model Station    MAE   NSE  MASE  MAE_HIGH\n",
      "0       LSTM-EncDec-Att-Fus   54001  23.87  0.71  1.12     95.92\n",
      "1       LSTM-EncDec-Att-Fus   54005  18.29  0.66  1.13     67.03\n",
      "2       LSTM-EncDec-Att-Fus   54032  32.95  0.78  1.15     96.23\n",
      "3       LSTM-EncDec-Att-Fus   54057  39.58  0.77  1.15    130.81\n",
      "4       LSTM-EncDec-Att-Fus   54095  25.79  0.65  1.08    124.13\n",
      "5       LSTM-EncDec-Att-Fus   55002  25.88  0.48  1.03    142.14\n",
      "6    LSTM-EncDec-Op-Lev-Fus   54001  22.82  0.72  1.07     93.27\n",
      "7    LSTM-EncDec-Op-Lev-Fus   54005  17.28  0.68  1.07     63.85\n",
      "8    LSTM-EncDec-Op-Lev-Fus   54032  31.50  0.79  1.10     96.86\n",
      "9    LSTM-EncDec-Op-Lev-Fus   54057  37.59  0.78  1.09    121.66\n",
      "10   LSTM-EncDec-Op-Lev-Fus   54095  23.65  0.67  0.99    120.78\n",
      "11   LSTM-EncDec-Op-Lev-Fus   55002  24.92  0.50  0.99    137.73\n",
      "12     1DCNN-BiLSTM-Att-Fus   54001  24.32  0.70  1.14     98.42\n",
      "13     1DCNN-BiLSTM-Att-Fus   54005  18.77  0.66  1.16     66.15\n",
      "14     1DCNN-BiLSTM-Att-Fus   54032  33.67  0.78  1.17     99.05\n",
      "15     1DCNN-BiLSTM-Att-Fus   54057  39.74  0.77  1.16    128.68\n",
      "16     1DCNN-BiLSTM-Att-Fus   54095  26.74  0.65  1.12    124.86\n",
      "17     1DCNN-BiLSTM-Att-Fus   55002  26.08  0.48  1.03    143.18\n",
      "18  1DCNN-BiLSTM-Op-Lev-Fus   54001  21.98  0.72  1.03     96.15\n",
      "19  1DCNN-BiLSTM-Op-Lev-Fus   54005  17.81  0.67  1.10     65.65\n",
      "20  1DCNN-BiLSTM-Op-Lev-Fus   54032  30.95  0.79  1.08     94.72\n",
      "21  1DCNN-BiLSTM-Op-Lev-Fus   54057  37.27  0.78  1.08    121.32\n",
      "22  1DCNN-BiLSTM-Op-Lev-Fus   54095  24.79  0.66  1.03    121.84\n",
      "23  1DCNN-BiLSTM-Op-Lev-Fus   55002  25.11  0.50  1.00    137.24\n"
     ]
    }
   ],
   "source": [
    "# Predictions per station\n",
    "num_stations = 6\n",
    "error_metrics = []\n",
    "\n",
    "models = [\n",
    "    \"LSTM-EncDec-Att-Fus\",\n",
    "    \"LSTM-EncDec-Op-Lev-Fus\"\n",
    "]\n",
    "\n",
    "for model_name, model in zip(models,predictions_per_model):\n",
    "  mae_high_per_station = []\n",
    "  mae_per_station = []\n",
    "  nse_per_station = []\n",
    "  mase_per_station = []\n",
    "  extreme_lows_per_station = []\n",
    "  extreme_highs_per_station = []\n",
    "  model =  np.array(model)\n",
    "  model = model.transpose(1,2,0).reshape(-1,6)\n",
    "\n",
    "  for i,name in zip(range(num_stations),weather_data.columns[7:]):\n",
    "\n",
    "      df = pd.DataFrame({'timestamp': range(0,len(actuals[:,i]+1)),'Flow': actuals[:,i]})\n",
    "\n",
    "      threshold_high = df['Flow'].quantile(0.95)  \n",
    "\n",
    "      extreme_highs = df[df['Flow'] >= threshold_high]\n",
    "\n",
    "      df2 = pd.DataFrame({'timestamp': range(0,len(actuals[:,i]+1)),'Flow': model[:,i].flatten()})\n",
    "\n",
    "      threshold_high2 = df['Flow'].quantile(0.95)  \n",
    "\n",
    "      extreme_highs2 = df2[df['Flow'] >= threshold_high2]\n",
    "\n",
    "      extreme_lows_per_station.append(extreme_lows)\n",
    "      extreme_highs_per_station.append(extreme_highs)\n",
    "\n",
    "      mae_high = round(mean_absolute_error(extreme_highs['Flow'], extreme_highs2['Flow']),2)\n",
    "      mae_low = round(mean_absolute_error(extreme_lows['Flow'], extreme_lows2['Flow']),2)\n",
    "      mae_high_per_station.append(mae_high)\n",
    "      mae = round(mean_absolute_error(np.array(df['Flow']), np.array(df2['Flow'])),2)\n",
    "      mae_per_station.append(mae)\n",
    "\n",
    "      nse = round(nash_sutcliffe_efficiency(np.array(df['Flow']), np.array(df2['Flow'])),2)\n",
    "      nse_per_station.append(nse)\n",
    "      mase = round(calculate_mase(np.array(df['Flow']), np.array(df2['Flow']),20),2)\n",
    "      mase_per_station.append(mase)\n",
    "\n",
    "      actual_quantiles_90 = df['Flow'].quantile(0.90)\n",
    "      predicted_quantiles_90 = df2['Flow'].quantile(0.90)\n",
    "      rqe_90 = round(np.abs(predicted_quantiles_90 - actual_quantiles_90) / actual_quantiles_90,2)\n",
    "      predicted_quantiles_95 = df2['Flow'].quantile(0.95)\n",
    "      rqe_95 = round(np.abs(predicted_quantiles_95 - actual_quantiles_95) / actual_quantiles_95,2)\n",
    "      actual_quantiles_99 = df['Flow'].quantile(0.99)\n",
    "      predicted_quantiles_99 = df2['Flow'].quantile(0.99)\n",
    "      rqe_99 = round(np.abs(predicted_quantiles_99 - actual_quantiles_99) / actual_quantiles_99,2)\n",
    "      actual_prob = df['Flow'] / np.sum(df['Flow'])\n",
    "      predicted_prob = df2['Flow'] / np.sum(df2['Flow'])\n",
    "      predicted_prob = np.clip(predicted_prob, 1e-10, None)  \n",
    "\n",
    "      quantile_95 = np.percentile(df2['Flow'], 95)\n",
    "      lower_bound = np.percentile(df2['Flow'], 90)  \n",
    "      upper_bound = quantile_95  \n",
    "      quantile_99 =   np.percentile(df2['Flow'], 99)\n",
    "      lower_bound_2 = np.percentile(df2['Flow'], 95) \n",
    "      upper_bound_2 = quantile_99 \n",
    "      lower_bound_3 = np.percentile(df2['Flow'], 10) \n",
    "      upper_bound_3 = lower_bound\n",
    "      # within_interval = (df2['Flow'] >= lower_bound) & (df2['Flow'] <= upper_bound)\n",
    "      within_interval_3 = (df['Flow'] >= lower_bound_3) & (df['Flow'] <= upper_bound_3)\n",
    "      within_interval_2 = (df['Flow'] >= lower_bound_2) & (df['Flow'] <= upper_bound_2)\n",
    "      within_interval = (df['Flow'] >= lower_bound) & (df['Flow'] <= upper_bound)\n",
    "      pci_3 = round(np.mean(within_interval_3) * 100,2)\n",
    "      pci_2 = round(np.mean(within_interval_2) * 100,2)\n",
    "      pci = round(np.mean(within_interval) * 100,2)\n",
    "\n",
    "      overlap = np.intersect1d(extreme_highs, extreme_highs2)\n",
    "      overlap_percentage = round(len(overlap) / len(extreme_highs) * 100,2)\n",
    "\n",
    "  error_metrics_df = pd.DataFrame({\n",
    "      'Model' : model_name,\n",
    "      'Station': [f'{name}' for name in weather_data.columns[7:]],\n",
    "      'MAE': mae_per_station,\n",
    "      'NSE' : nse_per_station,\n",
    "      'MASE': mase_per_station,\n",
    "      'MAE_HIGH': mae_high_per_station\n",
    "\n",
    "  })\n",
    "\n",
    "  error_metrics.append(error_metrics_df)\n",
    "\n",
    "error_metrics = pd.concat(error_metrics,axis=0).reset_index(drop=True)\n",
    "print(error_metrics)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predictions_per_model_un.npy',predictions_per_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
